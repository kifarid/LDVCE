{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "from torchvision import transforms, datasets\n",
    "import random\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the imagenet graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_node('A')\n",
    "G.add_node('B')\n",
    "G.add_node('C')\n",
    "G.add_node('D')\n",
    "\n",
    "# Add edges\n",
    "G.add_edge('A', 'B')\n",
    "G.add_edge('A', 'C')\n",
    "G.add_edge('B', 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#nx.draw_networkx(G, nx.spring_layout(G))\n",
    "G.remove_node('D')\n",
    "nx.draw_networkx(G, nx.spring_layout(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/imagenette2/synset_human.txt\", \"r\") as f:    \n",
    "    synset_human_complete = f.read().splitlines()\n",
    "    synset_human_complete = dict(line.split(maxsplit=1) for line in synset_human_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagenet_classnames import name_map, folder_label_map\n",
    "\n",
    "synset_human = folder_label_map\n",
    "index_human = name_map\n",
    "\n",
    "#check that synset_human keys are ordered\n",
    "for i, (key_syn, key_index) in enumerate(zip(sorted(synset_human.keys()), sorted(index_human.keys()))):\n",
    "    assert synset_human[key_syn] == index_human[key_index]\n",
    "    assert i == key_index\n",
    "\n",
    "index_synset = { i: k for i, k in enumerate(synset_human.keys())}\n",
    "\n",
    "# validate the index_synset with index to human\n",
    "for index in index_synset.keys():\n",
    "    assert index_human[index] == synset_human[index_synset[index]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wordnet.is_a.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "#convert from synset to human and save to file\n",
    "with open('data/wordnet.is_a_human.txt', 'w') as f:\n",
    "    for line in data.splitlines():\n",
    "        synset1, synset2 = line.split()\n",
    "        f.write(f'{synset_human_complete[synset2]} is a {synset_human_complete[synset1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty directed graph object\n",
    "G_hum = nx.DiGraph()\n",
    "G_syn = nx.DiGraph()\n",
    "\n",
    "# Open the file and read its contents\n",
    "with open('data/wordnet.is_a.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Split the data into lines and iterate over each line\n",
    "for line in data.split('\\n'):\n",
    "    # Split the line into parent and child node IDs\n",
    "    if len(line) > 0:\n",
    "        parent, child = line.split()\n",
    "        if parent in synset_human: \n",
    "            #print(\"parent is a leaf in imagenet\", parent, synset_human[parent], '\\n')\n",
    "            #print(\"the child is\", child, synset_human_complete[child], '\\n')\n",
    "            assert child not in synset_human, \"child is in imagenet\"\n",
    "            continue\n",
    "        # Add an edge between the parent and child nodes\n",
    "        G_hum.add_edge(synset_human_complete[parent], synset_human_complete[child])\n",
    "        G_syn.add_edge(parent, child)\n",
    "\n",
    "# Print the nodes and edges in the graph\n",
    "print(\"Nodes:\", sorted(G_syn.nodes()))\n",
    "print(sorted(synset_human.keys()))\n",
    "#print(\"Edges:\", G1.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(synset_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get duplicated human labels\n",
    "from collections import Counter\n",
    "duplicates = [k for k,v in Counter(synset_human.values()).items() if v>1]\n",
    "duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in synset_human.keys():\n",
    "    assert key in G_syn.nodes(), \"key is in imagenet but not in graph\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #remove the leaf nodes that are not in imagenet\n",
    "# for node in list(G1.nodes):\n",
    "#     if node not in human_synset and G1.out_degree(node) == 0:\n",
    "#         if node == \"volleyball player\":\n",
    "#             print(\"removing\", node, '\\n')\n",
    "#         G1.remove_node(node)\n",
    "\n",
    "# for node in list(G2.nodes):\n",
    "#     if node not in human_synset and G2.out_degree(node) == 0:\n",
    "#         G2.remove_node(node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recursively remove the leaf nodes that are not in imagenet\n",
    "def remove_leaf_nodes(G):\n",
    "    len_before = len(G.nodes())\n",
    "    for node in list(G.nodes()):\n",
    "        if G.out_degree(node) == 0 and node not in synset_human.values():\n",
    "            G.remove_node(node)\n",
    "            #print(\"removing node\", node)\n",
    "    len_after = len(G.nodes())\n",
    "    print(\"len before\", len_before, \"len after\", len_after)\n",
    "    \n",
    "    if len_before != len_after:\n",
    "        remove_leaf_nodes(G)\n",
    "    return G\n",
    "\n",
    "#recursively remove the leaf nodes that are not in imagenet\n",
    "def remove_leaf_node_syn(G):\n",
    "    len_before = len(G.nodes())\n",
    "    nodes_in_imagenet = [node for node in G.nodes() if node in synset_human]\n",
    "    nodes_not_in_imagenet = [node for node in G.nodes() if node not in synset_human]\n",
    "    total_nodes = len(nodes_in_imagenet)\n",
    "    print(\"total nodes in imagenet\", total_nodes)\n",
    "    print(\"nodes in imagenet\", len(nodes_in_imagenet))\n",
    "    for node in nodes_not_in_imagenet:\n",
    "        if G.out_degree(node) == 0:\n",
    "            G.remove_node(node)\n",
    "            #print(\"removing node\", node)\n",
    "    # for node in list(G.nodes()):\n",
    "\n",
    "    #     if G.out_degree(node) == 0 and node not in synset_human:\n",
    "    #         #print(\"removing node\", node, synset_human[node])\n",
    "    #         G.remove_node(node)\n",
    "    #         #print(\"removing node\", node)\n",
    "    len_after = len(G.nodes())\n",
    "    print(\"len before\", len_before, \"len after\", len_after)\n",
    "    \n",
    "    if len_before != len_after:\n",
    "        remove_leaf_node_syn(G)\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_hum_filtered = remove_leaf_nodes(G_hum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_syn_filtered = remove_leaf_node_syn(G_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_nodes =[node for node in list(G_syn.nodes) if G_syn.out_degree(node) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(leaf_nodes).intersection(set(synset_human.keys()))), len(leaf_nodes), len(synset_human.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_leaf_nodes(G, orig_node, parents = None, k=4, siblings_ordered=defaultdict(list)):\n",
    "    # if len(siblings_ordered[orig_node]) >= k:\n",
    "    #     print(\"returning top\")\n",
    "    #     return siblings_ordered[orig_node][:k]\n",
    "    # Get the parent node\n",
    "    if parents is None:\n",
    "        #print(\"parents is none\")\n",
    "        parents = list(G.predecessors(orig_node))\n",
    "        if len(parents) == 0:\n",
    "            #print(\"NO more parents\")\n",
    "            return None\n",
    "   # print(f\"Siblings ordered at this level:\", '\\n', f\"{[synset_human_complete[sibling] for sibling in siblings_ordered[orig_node]]}\")\n",
    "    #print(f\"current parents are: {[synset_human_complete[parent] for parent in parents]}\")\n",
    "    # Get all direct sibling leaf nodes\n",
    "    curr_siblings = []\n",
    "    for parent in parents:\n",
    "        #print(f\"Searching for Parent {synset_human_complete[parent]}:\", '\\n')\n",
    "        leaves_of_parent = [n for n in nx.dfs_preorder_nodes(G, parent) if G.out_degree(n) == 0 and n != orig_node and n not in siblings_ordered[orig_node]]\n",
    "        #print(f\"Leaves of Parent {synset_human_complete[parent]} are:\", '\\n')\n",
    "        #print(f\"{[synset_human_complete[leaf] for leaf in leaves_of_parent]}\")\n",
    "        curr_siblings.extend(leaves_of_parent)\n",
    "    #print(orig_node, curr_siblings)\n",
    "    #print(f\"Curr Siblings:\", '\\n', f\"{[synset_human_complete[sibling] for sibling in curr_siblings]}\")\n",
    "    distances = {}\n",
    "    for sibling in curr_siblings:\n",
    "        distances[sibling] = nx.shortest_path_length(G.to_undirected(as_view=True), orig_node, sibling)\n",
    "    \n",
    "    # Sort the siblings by distance and return the closest k\n",
    "    closest = sorted(distances, key=distances.get)[:k]\n",
    "    #print(f\"Curr Siblings sorted:\", '\\n', f\"{[synset_human_complete[sibling] for sibling in curr_siblings]}\")\n",
    "\n",
    "    siblings_ordered[orig_node].extend(closest)\n",
    "    #print(f\"Siblings ordered at this level:\", '\\n', f\"{[synset_human_complete[sibling] for sibling in siblings_ordered[orig_node]]}\")\n",
    "    \n",
    "    if len(siblings_ordered[orig_node]) >= k:\n",
    "        #print(\" len is greater than k\")\n",
    "        siblings_ordered[orig_node] = siblings_ordered[orig_node][:k]\n",
    "        #print(f\"returning {siblings_ordered[orig_node][:k]}\")\n",
    "        return None #siblings_ordered[orig_node][:k]\n",
    "    \n",
    "    else:\n",
    "        #print(\"exploring parents of parents\")\n",
    "        # If there are less than k siblings, go up to the parent's parent and try again\n",
    "        # get parents of parents\n",
    "        parents_of_parents = []\n",
    "        for parent in parents:\n",
    "            parents_of_parents.extend(list(G.predecessors(parent)))\n",
    "        \n",
    "        x = closest_leaf_nodes(G, orig_node, parents_of_parents, k, siblings_ordered)\n",
    "\n",
    "        return \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_leaf_nodes(G, orig_node, parents = None, k=4, siblings_ordered=defaultdict(list)):\n",
    "    if parents is None:\n",
    "        parents = list(G.predecessors(orig_node))\n",
    "        if len(parents) == 0:\n",
    "            return None\n",
    "    curr_siblings = []\n",
    "    for parent in parents:\n",
    "        leaves_of_parent = [n for n in nx.dfs_preorder_nodes(G, parent) if G.out_degree(n) == 0 and n != orig_node and n not in siblings_ordered[orig_node]]\n",
    "        curr_siblings.extend(leaves_of_parent)\n",
    "    distances = {}\n",
    "    for sibling in curr_siblings:\n",
    "        distances[sibling] = nx.shortest_path_length(G.to_undirected(as_view=True), orig_node, sibling)\n",
    " \n",
    "    closest = sorted(distances, key=distances.get)[:k]\n",
    " \n",
    "    siblings_ordered[orig_node].extend(closest)\n",
    "\n",
    "\n",
    "    if len(siblings_ordered[orig_node]) >= k:\n",
    "        siblings_ordered[orig_node] = siblings_ordered[orig_node][:k]\n",
    "        return None #siblings_ordered[orig_node][:k]\n",
    "    \n",
    "    else:\n",
    "\n",
    "        parents_of_parents = []\n",
    "        for parent in parents:\n",
    "            parents_of_parents.extend(list(G.predecessors(parent)))\n",
    "        \n",
    "        x = closest_leaf_nodes(G, orig_node, parents_of_parents, k, siblings_ordered)\n",
    "        return \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siblings_ordered = defaultdict(list)\n",
    "closest_leaf_nodes(G_syn_filtered, 'n01484850', k=4, siblings_ordered=siblings_ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(siblings_ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in siblings_ordered['n01484850']:\n",
    "    print(i)\n",
    "    print(synset_human_complete[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siblings_ordered = defaultdict(list)\n",
    "for i, node in enumerate(synset_human.keys(), 0):\n",
    "\n",
    "    closest_leaf_nodes(G_syn_filtered, node, k=4, siblings_ordered=siblings_ordered)\n",
    "    print(\"Node:\", synset_human_complete[node], node)\n",
    "    for sibling in siblings_ordered[node]:\n",
    "        print(\"Sibling:\", synset_human_complete[sibling])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the siblings_ordered dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in x:\n",
    "    print(synset_human_complete[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siblings_hum = {}\n",
    "for key, value in siblings_ordered.items():\n",
    "    siblings_hum[synset_human_complete[key]] = [synset_human_complete[i] for i in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invert the dict index_synset to synset_index\n",
    "synset_index = {v: k for k, v in index_synset.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siblings_idx = {}\n",
    "for key, value in siblings_ordered.items():\n",
    "    siblings_idx[synset_index[key]] = [synset_index[i] for i in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/image_idx_to_tgt_class_closest_5.yaml', 'w') as file:\n",
    "    documents = yaml.dump(dict(siblings_ordered), file)\n",
    "\n",
    "\n",
    "with open('data/image_idx_to_tgt_class_closest_5_human.yaml', 'w') as file:\n",
    "\n",
    "    documents = yaml.dump(dict(siblings_hum), file)\n",
    "\n",
    "with open('data/image_idx_to_tgt_class_closest_5_idx.yaml', 'w') as file:\n",
    "    \n",
    "        documents = yaml.dump(dict(siblings_idx), file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data from artifact "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('kifarid/cdiff/run-nfw1b6uy-dvce_video:v4', type='run_table')\n",
    "df = pd.DataFrame(data=artifact.get(\"dvce_video\").data, columns=artifact.get(\"dvce_video\").columns)\n",
    "memory_usage = df.memory_usage(deep=True).sum()\n",
    "print(f\"Memory usage of dataframe is {memory_usage/1e6} MB\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the tgt_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "with open('data/synset_closest_idx.yaml', 'r') as file:\n",
    "    synset_closest_idx = yaml.safe_load(file)\n",
    "\n",
    "\n",
    "data_path = '/misc/scratchSSD2/datasets/ILSVRC2012/val'\n",
    "out_size = 256\n",
    "transform_list = [\n",
    "    transforms.Resize((out_size,out_size)),\n",
    "    transforms.ToTensor()\n",
    "]\n",
    "transform = transforms.Compose(transform_list)\n",
    "dataset = datasets.ImageFolder(data_path,  transform=transform)\n",
    "\n",
    "idx_image_to_tgt_class = {}\n",
    "for i in range(len(dataset)):\n",
    "    img, label = dataset[i]\n",
    "    #print(synset_closest_idx[label], random.choice(synset_closest_idx[label]))\n",
    "    idx_image_to_tgt_class[i] = random.choice(synset_closest_idx[label])\n",
    "    if i%100==0:\n",
    "        print(f\"current image index: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert default dict to dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/image_idx_to_tgt_class.yaml', 'w') as file:\n",
    "    documents = yaml.dump(dict(idx_image_to_tgt_class), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the new Imagenet wrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/image_idx_to_tgt_class_closest_5.yaml', 'r') as file:\n",
    "    image_idx_to_tgt_class_closest_5 = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.imagenet_classnames import name_map, folder_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNet(datasets.ImageFolder):\n",
    "    classes = [name_map[i] for i in range(1000)]\n",
    "    name_map = name_map\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            root:str, \n",
    "            split:str=\"val\", \n",
    "            transform=None, \n",
    "            target_transform=None, \n",
    "            class_idcs=None, \n",
    "            start_sample: int = 0, \n",
    "            end_sample: int = 50000//1000,\n",
    "            return_tgt_cls: bool = False,\n",
    "            idx_to_tgt_cls_path = None, \n",
    "            **kwargs\n",
    "    ):\n",
    "        _ = kwargs  # Just for consistency with other datasets.\n",
    "        assert split in [\"train\", \"val\"]\n",
    "        assert start_sample < end_sample and start_sample >= 0 and end_sample <= 50000//1000\n",
    "        path = root if root[-3:] == \"val\" or root[-5:] == \"train\" else os.path.join(root, split)\n",
    "        super().__init__(path, transform=transform, target_transform=target_transform)\n",
    "        \n",
    "        with open(idx_to_tgt_cls_path, 'r') as file:\n",
    "            idx_to_tgt_cls = yaml.safe_load(file)\n",
    "            if isinstance(idx_to_tgt_cls, dict):\n",
    "                idx_to_tgt_cls = [idx_to_tgt_cls[i] for i in range(len(idx_to_tgt_cls))]\n",
    "        self.idx_to_tgt_cls = idx_to_tgt_cls\n",
    "\n",
    "        self.return_tgt_cls = return_tgt_cls\n",
    "\n",
    "        if class_idcs is not None:\n",
    "            class_idcs = list(sorted(class_idcs))\n",
    "            tgt_to_tgt_map = {c: i for i, c in enumerate(class_idcs)}\n",
    "            self.classes = [self.classes[c] for c in class_idcs]\n",
    "            samples = []\n",
    "            idx_to_tgt_cls = []\n",
    "            for i, (p, t) in enumerate(self.samples):\n",
    "                if t in tgt_to_tgt_map:\n",
    "                    samples.append((p, tgt_to_tgt_map[t]))\n",
    "                    idx_to_tgt_cls.append(self.idx_to_tgt_cls[i])\n",
    "            \n",
    "            self.idx_to_tgt_cls = idx_to_tgt_cls\n",
    "            #self.samples = [(p, tgt_to_tgt_map[t]) for i, (p, t) in enumerate(self.samples) if t in tgt_to_tgt_map]\n",
    "            self.class_to_idx = {k: tgt_to_tgt_map[v] for k, v in self.class_to_idx.items() if v in tgt_to_tgt_map}\n",
    "\n",
    "        if \"val\" == split: # reorder\n",
    "            new_samples = []\n",
    "            idx_to_tgt_cls = []\n",
    "            for idx in range(50000//1000):\n",
    "                new_samples.extend(self.samples[idx::50000//1000])\n",
    "                idx_to_tgt_cls.extend(self.idx_to_tgt_cls[idx::50000//1000])\n",
    "            self.samples = new_samples[start_sample*1000:end_sample*1000]\n",
    "            self.idx_to_tgt_cls = idx_to_tgt_cls[start_sample*1000:end_sample*1000]\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.class_labels = {i: folder_label_map[folder] for i, folder in enumerate(self.classes)}\n",
    "        self.targets = np.array(self.samples)[:, 1]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = super().__getitem__(index)\n",
    "        if self.return_tgt_cls:\n",
    "            return *sample, self.idx_to_tgt_cls[index]\n",
    "        else:\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_idx_to_tgt_class_closest_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dict to list \n",
    "image_idx_to_tgt_class_closest_5_list = [ image_idx_to_tgt_class_closest_5[i] for i in range(len(image_idx_to_tgt_class_closest_5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ImageNet('/misc/scratchSSD2/datasets/ILSVRC2012', idx_to_tgt_cls_path='data/image_idx_to_tgt.yaml', return_tgt_cls = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data_loader = torch.utils.data.DataLoader(ds, batch_size=2, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.classes[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[21][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.class_labels[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.classes[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size = 256\n",
    "transform_list = [\n",
    "    transforms.Resize((out_size, out_size)),\n",
    "    transforms.ToTensor()\n",
    "]\n",
    "transform = transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ImageNet('/misc/scratchSSD2/datasets/ILSVRC2012', split=\"val\", return_tgt_cls = True, idx_to_tgt_cls=image_idx_to_tgt_class_closest_5_list, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New cone projection approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image tench_eaten\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "path = 'tench_eaten.png'\n",
    "img = Image.open(path)\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cone_project_chuncked(grad_temp_1, grad_temp_2, deg, chunk_size = 2):\n",
    "    \"\"\"\n",
    "    grad_temp_1: gradient of the loss w.r.t. the robust/classifier free\n",
    "    grad_temp_2: gradient of the loss w.r.t. the non-robust\n",
    "    projecting the robust/CF onto the non-robust\n",
    "    \"\"\"\n",
    "    orig_shp = (grad_temp_1.shape[0], 3, int((grad_temp_1.shape[-1]//3)**(1/2) ), int((grad_temp_1.shape[-1]//3)**(1/2) ))\n",
    "    print(orig_shp)\n",
    "    grad_temp_1_chuncked = grad_temp_1.view(*orig_shp) \\\n",
    "    .unfold(2, chunck_size, chunck_size) \\\n",
    "    .unfold(3, chunck_size, chunck_size) \\\n",
    "    .permute(0, 1, 4, 5, 2, 3) \\\n",
    "    .reshape(orig_shp[0], -1, orig_shp[-2]//chunck_size, orig_shp[-1]//chunck_size) \\\n",
    "    .permute(0, 2, 3, 1)\n",
    "    \n",
    "    grad_temp_2_chuncked = grad_temp_2.view(*orig_shp) \\\n",
    "    .unfold(2, chunck_size, chunck_size) \\\n",
    "    .unfold(3, chunck_size, chunck_size) \\\n",
    "    .permute(0, 1, 4, 5, 2, 3) \\\n",
    "    .reshape(orig_shp[0], -1, orig_shp[-2]//chunck_size, orig_shp[-1]//chunck_size) \\\n",
    "    .permute(0, 2, 3, 1)\n",
    "   \n",
    "    print(grad_temp_1_chuncked.shape, grad_temp_2_chuncked.shape)\n",
    "    angles_before_chuncked = torch.acos((grad_temp_1_chuncked * grad_temp_2_chuncked).sum(-1) / (grad_temp_1_chuncked.norm(p=2, dim=-1) * grad_temp_2_chuncked.norm(p=2, dim=-1)))\n",
    "    #print('angle before', angles_before_chuncked)\n",
    "    grad_temp_2_chuncked_norm = grad_temp_2_chuncked / grad_temp_2_chuncked.norm(p=2, dim=-1).view(grad_temp_1_chuncked.shape[0], grad_temp_1_chuncked.shape[1], grad_temp_1_chuncked.shape[1], -1)\n",
    "    #print(f\" norm {grad_temp_2_chuncked_norm.norm(p=2, dim=-1) ** 2}\")\n",
    "    grad_temp_1_chuncked = grad_temp_1_chuncked - ((grad_temp_1_chuncked * grad_temp_2_chuncked_norm).sum(-1) / (grad_temp_2_chuncked_norm.norm(p=2, dim=-1) ** 2)).view(\n",
    "         grad_temp_1_chuncked.shape[0], grad_temp_1_chuncked.shape[1], grad_temp_1_chuncked.shape[1], -1) * grad_temp_2_chuncked_norm\n",
    "\n",
    "    grad_temp_1_chuncked_norm = grad_temp_1_chuncked / grad_temp_1_chuncked.norm(p=2, dim=-1).view(grad_temp_1_chuncked.shape[0], grad_temp_1_chuncked.shape[1], grad_temp_1_chuncked.shape[1], -1)\n",
    "    radians = torch.tensor([deg], device=grad_temp_1_chuncked.device).deg2rad()\n",
    "    cone_projection = grad_temp_2_chuncked.norm(p=2, dim=-1).unsqueeze(-1) * grad_temp_1_chuncked_norm * torch.tan(radians) + grad_temp_2_chuncked\n",
    "\n",
    "    # second classifier is a non-robust one -\n",
    "    # unless we are less than 45 degrees away - don't cone project\n",
    "    #print(\" ratio of dimensions that are cone projected: \", (angles_before > radians).float().mean())\n",
    "    #print(\"angle before\", angles_before.mean(), angles_before.std(), angles_before.min(), angles_before.max())\n",
    "    #print(\"radians\", radians)\n",
    "    print(grad_temp_2_chuncked)\n",
    "\n",
    "    grad_temp_chuncked = grad_temp_2_chuncked.clone()\n",
    "    print(angles_before_chuncked > radians, grad_temp_1_chuncked.shape)\n",
    "    grad_temp_chuncked[angles_before_chuncked > radians] = grad_temp_1_chuncked[angles_before_chuncked > radians] #cone_projection[angles_before_chuncked > radians]\n",
    "    print(grad_temp_chuncked.shape)\n",
    "\n",
    "    \n",
    "\n",
    "    grad_temp = grad_temp_chuncked.permute(0, 3, 1, 2) \\\n",
    "    .reshape(orig_shp[0], orig_shp[1], \n",
    "    chunck_size, chunck_size,\n",
    "    grad_temp_1_chuncked.shape[1], grad_temp_1_chuncked\n",
    "    .shape[2]) \\\n",
    "    .permute(0, 1, 4, 2, 5, 3) \\\n",
    "    .reshape(*(orig_shp))\n",
    "     \n",
    "    print(angles_before_chuncked.shape)\n",
    "\n",
    "    return grad_temp, angles_before_chuncked > radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_1 = torch.rand(1, 3, 4, 4).float()\n",
    "input_tensor_2 = torch.rand(1, 3, 4, 4).float()\n",
    "cone_project_chuncked(input_tensor_1.view(1, -1), input_tensor_2.view(1, -1), 45., chunk_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate stuff\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/misc/lmbraid21/faridk/ImageNetDVCEs_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "torch.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define columns for the wandb table\n",
    "columns = None\n",
    "data_dict = {}\n",
    "#gt_images = []\n",
    "#counterfactual_images = []\n",
    "diff = []\n",
    "# Loop through the files in the data directory\n",
    "for j, bucket in enumerate(sorted(os.listdir(data_dir))):\n",
    "    # if j == 9:\n",
    "    #     break\n",
    "    if os.path.isdir(os.path.join(data_dir, bucket)):\n",
    "        print(\"logging bucket: \", bucket, \" to wandb ...\")\n",
    "        for i, filename in enumerate(sorted(os.listdir(os.path.join(data_dir, bucket)))):\n",
    "            if filename.endswith(\".pth\")  and filename.split('.')[0].isdigit():\n",
    "                print(\"logging file: \", filename, \" to wandb ...\")\n",
    "                # Load the data from the file\n",
    "                data = torch.load(os.path.join(data_dir, bucket, filename))\n",
    "\n",
    "\n",
    "                # Add data to the data dictionary except for the image data\n",
    "                for key, value in data.items():\n",
    "                    #remove columns that have the substring 'video' or 'cgs' in them\n",
    "                    if 'video' in key or 'cgs' in key or 'image' in key or 'img' in key or 'counterfactual' in key:\n",
    "                        if key == 'counterfactual':\n",
    "                            pass\n",
    "                            #counterfactual_images=value\n",
    "                        elif 'img' in key:\n",
    "                            pass\n",
    "                            #gt_images =value \n",
    "                        continue\n",
    "                    else:\n",
    "                        pass \n",
    "                        # if key not in data_dict:\n",
    "                        #     #pass\n",
    "                        #     data_dict[key] = []\n",
    "                        # data_dict[key].append(value)\n",
    "                diff.append(gt_images - counterfactual_images)\n",
    "            #break\n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df) #.head()\n",
    "\n",
    "\n",
    "#rename in_probability to max_probability\n",
    "#df.rename(columns={'in_probability': 'max_probability'}, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tgt_prob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tgt_prob\"] = df.apply(lambda x: x['out_probability'][x['target']].item(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"max_prob\"] = df.apply(lambda x: x['out_probability'].max().item(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[\"tgt_prob\"] == df[\"max_prob\"]).mean(), df[\"tgt_prob\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_imgs = torch.stack(gt_images, dim=0)\n",
    "cf_imgs = torch.stack(counterfactual_images, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the first 3 images\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cf_imgs[2].permute(1, 2, 0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = gt_imgs-cf_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = torch.stack(diff, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff =  (init_image - all_samples[j][1:])\n",
    "\n",
    "diff = diff.view(diff.shape[0], -1)\n",
    "lp1 = int(torch.norm(diff, p=1, dim=-1).mean().cpu().numpy())\n",
    "lp2 = int(torch.norm(diff, p=2, dim=-1).mean().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp1, lp2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average(file_path, num_lines=20):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    print(lines)\n",
    "    #drop duplicates\n",
    "    #lines = list(set(lines))\n",
    "    progress_lines = [line.strip() for i, line in enumerate(lines) if '100%' in line and '100%' in lines[i]]\n",
    "    progress_lines = progress_lines[:num_lines]\n",
    "\n",
    "    numbers = []\n",
    "    for line in progress_lines:\n",
    "        start_index = line.find('[') + 1\n",
    "        end_index = line.find('<')\n",
    "        print(line[start_index:end_index], '\\n')\n",
    "        #the time is in format MM:SS\n",
    "\n",
    "        time_string = line[start_index:end_index]\n",
    "        minutes, seconds = time_string.split(':')\n",
    "        seconds = float(seconds) + float(minutes) * 60\n",
    "        #seconds = float(time_string[:-1])\n",
    "        numbers.append(seconds)\n",
    "\n",
    "    average = sum(numbers) / len(numbers)\n",
    "    return average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_average('logs/ldvces_cc_l1_sd.o2492461-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_dvce(file_path, num_lines=20):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    print(lines)\n",
    "    #drop duplicates\n",
    "    #lines = list(set(lines))\n",
    "    progress_lines = [line.strip() for i, line in enumerate(lines) if '100%' in line]\n",
    "    progress_lines = progress_lines[:num_lines]\n",
    "\n",
    "    numbers = []\n",
    "    for line in progress_lines:\n",
    "        start_index = line.find('[') + 1\n",
    "        end_index = line.find('<')\n",
    "        print(line[start_index:end_index], '\\n')\n",
    "        #the time is in format MM:SS\n",
    "\n",
    "        time_string = line[start_index:end_index]\n",
    "        minutes, seconds = time_string.split(':')\n",
    "        seconds = float(seconds) + float(minutes) * 60\n",
    "        #seconds = float(time_string[:-1])\n",
    "        numbers.append(seconds)\n",
    "\n",
    "    average = sum(numbers) / len(numbers)\n",
    "    return average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_average('/misc/student/faridk/DVCEs/logs/DVCEs_experiment.o2491648')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging bucket:  bucket_0_7  to wandb ...\n",
      "logging file:  00000.pth  to wandb ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlogging file: \u001b[39m\u001b[39m\"\u001b[39m, filename, \u001b[39m\"\u001b[39m\u001b[39m to wandb ...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[39m# Load the data from the file\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_dir, bucket, filename))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = '/misc/lmbraid21/faridk/ldvce_pets_42_24/'\n",
    "# Define columns for the wandb table\n",
    "columns = None\n",
    "data_dict = {}\n",
    "#gt_images = []\n",
    "#counterfactual_images = []\n",
    "diff = []\n",
    "# Loop through the files in the data directory\n",
    "for j, bucket in enumerate(sorted(os.listdir(data_dir))):\n",
    "    if bucket=='bucket_0_7':\n",
    "\n",
    "        # if j == 9:\n",
    "        #     break\n",
    "        if os.path.isdir(os.path.join(data_dir, bucket)):\n",
    "            print(\"logging bucket: \", bucket, \" to wandb ...\")\n",
    "            for i, filename in enumerate(sorted(os.listdir(os.path.join(data_dir, bucket)))):\n",
    "                if filename.endswith(\".pth\")  and filename.split('.')[0].isdigit():\n",
    "                    print(\"logging file: \", filename, \" to wandb ...\")\n",
    "                    # Load the data from the file\n",
    "                    data = torch.load(os.path.join(data_dir, bucket, filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/student/faridk/miniconda3/envs/ldm_fin/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.fig_utils import get_concat_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [\n",
    "  8970,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of the indices and the corresponding images\n",
    "indices_dict = {i:idx for idx, i in enumerate(indices)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8970: 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/misc/lmbraid21/faridk/LDCE_w382_cc23\"\n",
    "base_path_sd = \"/misc/lmbraid21/faridk/LDCE_sd_figures\"\n",
    "base_path_svce = \"/misc/lmbraid21/faridk/ImageNetSVCEs_robustOnly\"\n",
    "base_path_dvce = \"/misc/lmbraid21/faridk/ImageNetDVCEs_\"\n",
    "save_path = \"/misc/lmbraid21/faridk/imagenet_comparison_neww_single\"\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "os.chmod(save_path, 0o777)\n",
    "\n",
    "bucket_folders = sorted(glob.glob(base_path + \"/bucket*\"))\n",
    "bucket_folder_sd = base_path_sd + \"/bucket_0_10\"\n",
    "bucket_folders_svce = sorted(glob.glob(base_path_svce + \"/bucket*\"))\n",
    "bucket_folders_dvce = sorted(glob.glob(base_path_dvce + \"/bucket*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = indices[0]\n",
    "bucket_idx = idx // 1000\n",
    "bucket_folder = bucket_folders[bucket_idx]\n",
    "original_img = Image.open(os.path.join(bucket_folder, \"original\", f\"{filename}.png\"))\n",
    "filename = str(idx).zfill(5)\n",
    "data = torch.load(os.path.join(bucket_folder, f\"{filename}.pth\"), map_location=\"cpu\")\n",
    "source, target = data[\"source\"], data[\"target\"]\n",
    "source = source.split(\",\")[0]\n",
    "target = target.split(\",\")[0]\n",
    "bucket_folder = bucket_folder_sd\n",
    "filename_sd =  str(12).zfill(5)\n",
    "counterfactual_img_sd = Image.open(os.path.join(bucket_folder, \"counterfactual\", f\"{filename_sd}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('alp', 'coral reef')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [original_img, counterfactual_img_sd]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfilepath = os.path.join(save_path, f\"{filename}_{source}_{target}.jpg\")\n",
    "get_concat_h(*imgs).save(outfilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    }
   ],
   "source": [
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "os.chmod(save_path, 0o777)\n",
    "\n",
    "#indices = range(10000)\n",
    "for ii, idx in enumerate(tqdm(indices, leave=False)):\n",
    "    bucket_idx = idx // 1000\n",
    "    bucket_folder = bucket_folders[bucket_idx]\n",
    "\n",
    "    filename = str(idx).zfill(5)\n",
    "    data = torch.load(os.path.join(bucket_folder, f\"{filename}.pth\"), map_location=\"cpu\")\n",
    "    source, target = data[\"source\"], data[\"target\"]\n",
    "    source = source.split(\",\")[0]\n",
    "    target = target.split(\",\")[0]\n",
    "\n",
    "    original_img = Image.open(os.path.join(bucket_folder, \"original\", f\"{filename}.png\"))\n",
    "    counterfactual_img = Image.open(os.path.join(bucket_folder, \"counterfactual\", f\"{filename}.png\"))\n",
    "\n",
    "    bucket_folder = bucket_folder_sd\n",
    "    filename_sd =  str(ii).zfill(5)\n",
    "    counterfactual_img_sd = Image.open(os.path.join(bucket_folder, \"counterfactual\", f\"{filename_sd}.png\"))\n",
    "\n",
    "    bucket_folder = bucket_folders_svce[bucket_idx]\n",
    "    counterfactual_svce = Image.open(os.path.join(bucket_folder, \"counterfactual\", f\"{filename}.png\"))\n",
    "\n",
    "    bucket_folder = bucket_folders_dvce[bucket_idx]\n",
    "    counterfactual_dvce = Image.open(os.path.join(bucket_folder, \"counterfactual\", f\"{filename}.png\"))\n",
    "\n",
    "    imgs = [original_img, counterfactual_svce, counterfactual_dvce, counterfactual_img, counterfactual_img_sd]\n",
    "\n",
    "    #outfilepath = os.path.join(save_path, f\"{filename}_{source}_{target}.png\")\n",
    "    #get_concat_h(original_img, counterfactual_img).save(outfilepath, dpi=(200, 200))\n",
    "    outfilepath = os.path.join(save_path, f\"{filename}_{source}_{target}.jpg\")\n",
    "    get_concat_h(*imgs).save(outfilepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing celebhq quickly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/student/faridk/miniconda3/envs/ldm_fin/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from torchvision import datasets, transforms\n",
    "from data.imagenet_classnames import name_map, folder_label_map\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import random\n",
    "import pickle\n",
    "import linecache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebAHQDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        data_dir,\n",
    "        partition,\n",
    "        image_idcs=None,\n",
    "        shard=0,\n",
    "        num_shards=1,\n",
    "        class_cond=False,\n",
    "        random_crop=True,\n",
    "        random_flip=True,\n",
    "        query_label=-1,\n",
    "        normalize=True,\n",
    "        restart_idx: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        from io import StringIO\n",
    "        # read annotation files\n",
    "        with open(os.path.join(data_dir, 'CelebAMask-HQ-attribute-anno.txt'), 'r') as f:\n",
    "            datastr = f.read()[6:]\n",
    "            datastr = 'idx ' +  datastr.replace('  ', ' ')\n",
    "\n",
    "        with open(os.path.join(data_dir, 'CelebA-HQ-to-CelebA-mapping.txt'), 'r') as f:\n",
    "            mapstr = f.read()\n",
    "            mapstr = [i for i in mapstr.split(' ') if i != '']\n",
    "\n",
    "        mapstr = ' '.join(mapstr)\n",
    "\n",
    "        data = pd.read_csv(StringIO(datastr), sep=' ')\n",
    "        partition_df = pd.read_csv(os.path.join(data_dir, 'list_eval_partition.csv'))\n",
    "        mapping_df = pd.read_csv(StringIO(mapstr), sep=' ')\n",
    "        # mapping_df.rename(columns={'orig_file': 'image_id'}, inplace=True)\n",
    "        partition_df = pd.merge(mapping_df, partition_df, on='idx')\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        if partition == 'train':\n",
    "            partition = 0\n",
    "        elif partition == 'val':\n",
    "            partition = 1\n",
    "        elif partition == 'test':\n",
    "            partition = 2\n",
    "        else:\n",
    "            raise ValueError(f'Unkown partition {partition}')\n",
    "\n",
    "        self.data = data[partition_df['split'] == partition]\n",
    "        self.data = self.data[shard::num_shards]\n",
    "        self.data.reset_index(inplace=True)\n",
    "        self.data.replace(-1, 0, inplace=True)\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.RandomHorizontalFlip() if random_flip else lambda x: x,\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.RandomResizedCrop(image_size, (0.95, 1.0)) if random_crop else lambda x: x,\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                 [0.5, 0.5, 0.5])  if normalize else lambda x: x\n",
    "        ])\n",
    "\n",
    "        self.query = query_label\n",
    "        self.class_cond = class_cond\n",
    "\n",
    "        self.restart_idx = restart_idx\n",
    "        if self.restart_idx > 0:\n",
    "            self.data = self.data.iloc[self.restart_idx:]\n",
    "            self.data.reset_index(inplace=True)\n",
    "            self.data.replace(-1, 0, inplace=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx, :]\n",
    "        labels = sample[2:].to_numpy()\n",
    "        if self.query != -1:\n",
    "            labels = int(labels[self.query])\n",
    "        else:\n",
    "            labels = torch.from_numpy(labels.astype('float32'))\n",
    "        img_file = sample['idx']\n",
    "\n",
    "        with open(os.path.join(self.data_dir, 'CelebA-HQ-img', img_file), \"rb\") as f:\n",
    "            img = Image.open(f)\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        img = self.transform(img)\n",
    "\n",
    "        if self.query != -1:\n",
    "            return img, labels, self.restart_idx + idx\n",
    "\n",
    "        if self.class_cond:\n",
    "            return img, labels, self.restart_idx + idx\n",
    "        else:\n",
    "            return img, {}, self.restart_idx + idx\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
